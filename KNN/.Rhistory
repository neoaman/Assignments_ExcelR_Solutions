set.seed(101)
split_f <- sample(nrow(forest),0.3*nrow(forest))
train_forest <- dum_fire[-split_f,]
test_forest <- dum_fire[split_c,]
test_forest
# Model Fitting
set.seed(101);fo_model_1 <- neuralnet(strength~.,data = train_forest)
# Model Fitting
set.seed(101);fo_model_1 <- neuralnet(area~.,data = train_forest)
str(fo_model_1)
plot(fo_model_1, rep = "best")
summary(con_model_1)
summary(fo_model_1)
NeuralNetTools::plotnet(fo_model_1,alpha=0.3)
Pred_fo_1 <- compute(fo_model_1,test_concret)
Pred_fo_1 <- compute(fo_model_1,test_forest)
Pred_fo_1
cor(Pred_fo_1$net.result,test_concret$strength) # 0.8153639
cor(Pred_fo_1$net.result,test_forest$strength) # 0.8153639
Pred_fo_1$net.result
test_forest$strength
cor(Pred_fo_1$net.result,test_forest$area) # 0.8153639
cor(Pred_fo_1$net.result,test_forest$area) # 0.8153639
Pred_fo_1 <- compute(fo_model_1,test_forest)
cor(Pred_fo_1$net.result,test_forest$area) # 0.8153639
# Model Fitting
set.seed(101);fo_model_1 <- neuralnet(area~.,data = train_forest)
str(fo_model_1)
plot(fo_model_1, rep = "best")
summary(fo_model_1)
NeuralNetTools::plotnet(fo_model_1,alpha=0.3)
Pred_fo_1 <- compute(fo_model_1,test_forest)
Pred_fo_1
cor(Pred_fo_1$net.result,test_forest$area) # 0.8153639
test_forest$area
Pred_fo_1$net.result
cor(Pred_fo_1$net.result,test_forest$area) # 0.8153639
# Model Fitting
set.seed(101);fo_model_1 <- neuralnet(area~.,data = train_forest,hidden = 5)
str(fo_model_1)
plot(fo_model_1, rep = "best")
summary(fo_model_1)
NeuralNetTools::plotnet(fo_model_1,alpha=0.3)
Pred_fo_1 <- compute(fo_model_1,test_forest)
cor(Pred_fo_1$net.result,test_forest$area) # 0.8153639
str(forest)
forest$area
plot(Pred_fo_1$net.result,test_forest$area)
Act_Pred_c1 <- denormalize(Pred_con_1$net.result,max(concret$strength),min(concret$strength))
cor(Act_Pred_c1,test_concret$strength) # 0.8153639
Act_Pred_f1 <- denormalize(Pred_fo_1$net.result,max(forest$area),min(forest$area))
cor(Act_Pred_f1,test_forest$area) # 0.8153639
runUrl("https://amanneo.shinyapps.io/airquality/",filetype = .R)
library(shiny)
runUrl("https://amanneo.shinyapps.io/airquality/",filetype = .R)
runUrl("https://amanneo.shinyapps.io/airquality/",filetype = .R)
runGitHub(repo = "airquality",username = "neoaman")
install.packages("shinydashboard")
runUrl("https://amanneo.shinyapps.io/airquality/",filetype = .R)
runGitHub(repo = "airquality",username = "neoaman")
# Findout accuracies we can see
mean(clustlist_O$single==df_norm$Type);mean(clustlist_O$average==df_norm$Type);mean(clustlist_O$mcquitty==df_norm$Type);mean(clustlist_O$ward.D==df_norm$Type)
setwd("D:\\STUDY PROCESS\\Excelr\\Assignments\\Pending\\PCA")
df <- read.csv("wine.csv")
dim(df)
str(df)
head(df)
table(df$Type)
#____________________________________MY FUNCTIONS____________________________________
normalize_dummy <- function(x){
col <- ncol(x)
row <- nrow(x)
y <- 1:nrow(x)
for (i in 1:col){
if(class(x[,i])=="numeric" | class(x[,i])=="integer")
{
minx <- min(x[,i])
maxx <- max(x[,i])
for(j in 1:row)
{
x[j,i] <- ifelse((x[j,i] - minx) != 0,yes =((x[j,i] - minx) / (maxx - minx)),no = 0)
}
}
}
f <- c()
for(i in 1:ncol(x)){
if(class(x[,i])=="factor"){
dummies <- data.frame(dummies::dummy(x[,i]))
y <- data.frame(y,dummies)
f <- c(f,i)
}
else{
next
}
}
if(is.null(f)){
output <- x
}
else{output <- data.frame(x[,-f],y[,-1])}
return(output)
}
all_hclust <- function(dist,k=3,method="all"){
method=c("single", "complete", "average", "mcquitty", "ward.D", "ward.D2", "centroid","median")
row <- 0
clust_df <<- as.data.frame(method,matrix(0,nrow = length(method),ncol = k))
all_clus <- list()
for(i in method){
row <- row+1
hcl <- hclust(d = dist,method = i)
clusters <- cutree(tree = hcl,k = k)
all_clus[[i]] <- clusters
clust_df[row,2:(k+1)] <- as.integer(tabulate(clusters))
}
all_clus[["table"]] <- clust_df
return(all_clus)
}
df_norm <- data.frame(Type=df$Type,normalize_dummy(df[,-1]))
head(df_norm)
#_______________________________________PCA_________________________________
# Performing PCA for this data
df_pca<-princomp(x = df_norm[,-1], cor = TRUE, scores = TRUE, covmat = NULL)
df_pca_2 <- df_pca$scores[,1:3]
#______________________________HIERARCHICAL CLUSTERING________________________________________________
# Perform Cluster analysis in original data
dist_O <-dist(df_norm,method = "euclidian")
#______________________________KMEANS CLUSTERING________________________________________________________
# Perform Kmeans in Original Data
Twss<-c();k <- 1:7
for(i in 1:7){
set.seed(33) # seed = 33  ; I set this seed for constant plot for every iteration so please dont remove it
km_a <- kmeans(x = df_norm,i)
Twss[i]<-km_a$tot.withinss
}
Twss
plot(y=Twss,x = k,"b",
col=c(rep(1,2),3,rep(1,4)),cex = c(rep(1,2),2,rep(1,4)),lwd=3,main = 'Scree Plot',
col.main="blue",col.axis="blue") # I may consider 3 as my optimum k value
km_a <- kmeans(x = df_norm,3)
km_o <- kmeans(x = df_norm,3)
km_o
mean(km_o==df_norm$Type)
km_o
mean(km_o$cluster==df_norm$Type)
km_o$cluster
df_norm$Type
table(km_o$cluster)
table(df_norm$Type)
table(km_o$cluster,df_norm$Type)
# Perform Kmeans in PCA Data
Twss<-c();k <- 1:7
for(i in 1:7){
set.seed(33) # seed = 33  ; I set this seed for constant plot for every iteration so please dont remove it
km_a <- kmeans(x = df_pca_2,i)
Twss[i]<-km_a$tot.withinss
}
Twss
plot(y=Twss,x = k,"b",
col=c(rep(1,2),3,rep(1,4)),cex = c(rep(1,2),2,rep(1,4)),lwd=3,main = 'Scree Plot',
col.main="blue",col.axis="blue") # I may consider 3 as my optimum k value
km_PCA <- kmeans(x = df_pca_2,3)
table(km_PCA$cluster)
table(df_pca_2$Type)
df_pca_2
df_pca_2
table(km_PCA$cluster)
table(df_norm$Type)
mean(km_PCA$cluster==df_norm$Type)
df_norm
mean(km_PCA$cluster==df_norm$Type)
table(km_PCA$cluster,df_norm$Type)
table(km_PCA$cluster)
table(df_norm$Type)
clust <- ifelse(km_PCA$cluster == 1,3,ifelse(km_PCA$cluster==2,1,2))
mean(clust==df_norm$Type)
table(clust,df_norm$Type)
table(km_PCA$cluster,df_norm$Type)
mean(clust==df_norm$Type)
table(km_o$cluster,df_norm$Type)
#______________________________KMEANS CLUSTERING________________________________________________________
# Perform Kmeans in Original Data
Twss<-c();k <- 1:7
for(i in 1:7){
set.seed(33) # seed = 33  ; I set this seed for constant plot for every iteration so please dont remove it
km_a <- kmeans(x = df_norm[,-1],i)
Twss[i]<-km_a$tot.withinss
}
Twss
plot(y=Twss,x = k,"b",
col=c(rep(1,2),3,rep(1,4)),cex = c(rep(1,2),2,rep(1,4)),lwd=3,main = 'Scree Plot',
col.main="blue",col.axis="blue") # I may consider 3 as my optimum k value
df_norm[,-1]
km_o <- kmeans(x = df_norm[,-1],3)
table(km_o$cluster)
table(df_norm$Type)
table(km_o$cluster,df_norm$Type)
mean(km_o$cluster==df_norm$Type)
table(km_o$cluster,df_norm$Type)
# It seems like Clustering is good but allocation of number is differing only
clusto <- ifelse(km_PCA$cluster == 1,3,ifelse(km_PCA$cluster==2,1,2))
mean(clusto==df_norm$Type)
set.seed(33);km_PCA <- kmeans(x = df_pca_2,3)
table(km_PCA$cluster)
table(df_norm$Type)
table(km_PCA$cluster,df_norm$Type)
clust <- ifelse(km_PCA$cluster == 1,3,ifelse(km_PCA$cluster==2,1,2))
mean(clust==df_norm$Type) # Efficiency is 0.9606742
table(clust,df_norm$Type)
set.seed(33);km_o <- kmeans(x = df_norm[,-1],3)
table(km_o$cluster)
table(df_norm$Type)
table(km_o$cluster,df_norm$Type)
# It seems like Clustering is good but allocation of number is differing only
clusto <- ifelse(km_PCA$cluster == 1,3,ifelse(km_PCA$cluster==2,1,2))
mean(clusto==df_norm$Type) # 0.9606742
clust
clusto
# Now comparing between the two model (PCA and Original)
mean(clust==clusto)
table(glass$Type)
setwd("D:\\STUDY PROCESS\\Excelr\\Assignments\\Pending\\KNN")
# Question no 1 ----
"Prepare a model for glass classification using KNN"
glass <- read.csv("glass.csv")
setwd("D:\\STUDY PROCESS\\Excelr\\Assignments\\Pending\\KNN")
# Question no 1 ----
"Prepare a model for glass classification using KNN"
glass <- read.csv("glass.csv")
table(glass$Type)
str(glass)
df_glass <- data.frame(glass[,-10],type=factor(glass$Type))
head(df_glass)
round(prop.table(table(glass$Type))*100,2)
summary(glass[,-10])
write.csv(summary(glass[,-10]),"out/summaryglass.csv")
glass
round(prop.table(table(glass$Type))*100,2)
df_glass <- data.frame(normalize_dummy(glass[,-10]),type=glass$Type)
df_glass
set.seed(101)
split <- sample(x =1:nrow(df_glass) ,size = round(nrow(glass)*.3),replace = F)
train_df <- df_glass[-split,];dim(train_df)
test_df <- df_glass[split,];dim(test_df)
split
df_glass <- data.frame(normalize_dummy(glass[,-10]),type=glass$Type)
set.seed(101)
split <- sample(x =1:nrow(df_glass) ,size = round(nrow(glass)*.3),replace = F)
train_df <- df_glass[-split,];dim(train_df)
test_df <- df_glass[split,];dim(test_df)
train_df
model_knn1 <- knn3(x = train_df[,-10],y = factor(train_df$type),k = 5)
model_knn1 <- knn3(x = train_df[,-10],y = factor(train_df$type),k = 5)
setwd("D:\\STUDY PROCESS\\Excelr\\Assignments\\Pending\\KNN")
# Question no 1 ----
"Prepare a model for glass classification using KNN"
glass <- read.csv("glass.csv")
table(glass$Type)
str(glass)
df_glass <- data.frame(glass[,-10],type=factor(glass$Type))
head(df_glass)
round(prop.table(table(glass$Type))*100,2)
summary(glass[,-10])
#write.csv(summary(glass[,-10]),"out/summaryglass.csv")
normalize_dummy <- function(x){
col <- ncol(x)
row <- nrow(x)
y <- 1:nrow(x)
for (i in 1:col){
if(class(x[,i])=="numeric" | class(x[,i])=="integer")
{
minx <- min(x[,i])
maxx <- max(x[,i])
for(j in 1:row)
{
x[j,i] <- ifelse((x[j,i] - minx) != 0,yes =((x[j,i] - minx) / (maxx - minx)),no = 0)
}
}
}
f <- c()
for(i in 1:ncol(x)){
if(class(x[,i])=="factor"){
dummies <- data.frame(dummies::dummy(x[,i]))
y <- data.frame(y,dummies)
f <- c(f,i)
}
else{
next
}
}
if(is.null(f)){
output <- x
}
else{output <- data.frame(x[,-f],y[,-1])}
return(output)
}
df_glass <- data.frame(normalize_dummy(glass[,-10]),type=glass$Type)
set.seed(101)
split <- sample(x =1:nrow(df_glass) ,size = round(nrow(glass)*.3),replace = F)
train_df <- df_glass[-split,];dim(train_df)
test_df <- df_glass[split,];dim(test_df)
# Method 1 ----
library(caret)
model_knn1 <- knn3(x = train_df[,-10],y = factor(train_df$type),k = 5)
model_knn1
yhat <- round(predict(model_knn1,newdata = test_df[,-10]))
pred_1 <- c()
for (i in 1:nrow(yhat)){
for(j in 1:ncol(yhat)){
if(yhat[i,j] == 1){
pred_1[i] <- as.numeric(colnames(yhat)[j])
}
}
}
table(test_df$type,pred_1)
mean(test_df$type==pred_1,na.rm=T) # 0.6491228
library("class")
df_glass
set.seed(2) # I am fixing the rendomstate to an arbitrary number i.e. 2
model_2 <- knn(train = train_df[,-10],test = test_df[,-10],cl = factor(train_df$type),k = 5)
pred_2 <- model_2
confusionmatrix2 <- table(test_df$type,model_2)
mean(test_df$type==model_2) # Accuracy is 0.625
library("class")
set.seed(2) # I am fixing the rendomstate to an arbitrary number i.e. 2
model_2 <- knn(train = train_df[,-10],test = test_df[,-10],cl = factor(train_df$type),k = 5)
pred_2 <- model_2
confusionmatrix2 <- table(test_df$type,model_2)
mean(test_df$type==model_2) # Accuracy is 0.625
confusionmatrix2
plot(seq(1,50,2),train_acc,col="red",type='b',pch=20,main="Train and Test  Accuracy",xlab = "K value",ylab = "Accuracy")
lines(seq(1,50,2),test_acc,col=ifelse(test_acc==max(test_acc),"blue","green"),type='b',pch=20,cex=ifelse(test_acc==max(test_acc),2,1))
# From the above two methods we can perform any
train_acc <- c();test_acc <- c()
for (i in seq(1,50,2)){
# I am fixing the rendomstate to an arbitrary number i.e. 3 dont remove it, it may lead to different answers
set.seed(2);pred_knns <- knn(train=train_df[,-10],test=train_df[,-10],cl=train_df$type,k=i)
train_acc <- c(train_acc,mean(train_df$type==pred_knns))
set.seed(2);perd_test_knn <- knn(train = train_df[,-10], test = test_df[,-10], cl = train_df$type, k=i)
test_acc <- c(test_acc,mean(perd_test_knn==test_df$type))
};test_acc
setwd("D:\\STUDY PROCESS\\Excelr\\Assignments\\Pending\\KNN")
# Question no 1 ----
"Prepare a model for glass classification using KNN"
glass <- read.csv("glass.csv")
table(glass$Type)
str(glass)
df_glass <- data.frame(glass[,-10],type=factor(glass$Type))
head(df_glass)
round(prop.table(table(glass$Type))*100,2)
summary(glass[,-10])
#write.csv(summary(glass[,-10]),"out/summaryglass.csv")
normalize_dummy <- function(x){
col <- ncol(x)
row <- nrow(x)
y <- 1:nrow(x)
for (i in 1:col){
if(class(x[,i])=="numeric" | class(x[,i])=="integer")
{
minx <- min(x[,i])
maxx <- max(x[,i])
for(j in 1:row)
{
x[j,i] <- ifelse((x[j,i] - minx) != 0,yes =((x[j,i] - minx) / (maxx - minx)),no = 0)
}
}
}
f <- c()
for(i in 1:ncol(x)){
if(class(x[,i])=="factor"){
dummies <- data.frame(dummies::dummy(x[,i]))
y <- data.frame(y,dummies)
f <- c(f,i)
}
else{
next
}
}
if(is.null(f)){
output <- x
}
else{output <- data.frame(x[,-f],y[,-1])}
return(output)
}
df_glass <- data.frame(normalize_dummy(glass[,-10]),type=glass$Type)
set.seed(101)
split <- sample(x =1:nrow(df_glass) ,size = round(nrow(glass)*.3),replace = F)
train_df <- df_glass[-split,];dim(train_df)
test_df <- df_glass[split,];dim(test_df)
# Method 1 ----
library(caret)
model_knn1 <- knn3(x = train_df[,-10],y = factor(train_df$type),k = 5)
yhat <- round(predict(model_knn1,newdata = test_df[,-10]))
pred_1 <- c()
for (i in 1:nrow(yhat)){
for(j in 1:ncol(yhat)){
if(yhat[i,j] == 1){
pred_1[i] <- as.numeric(colnames(yhat)[j])
}
}
}
table(test_df$type,pred_1)
mean(test_df$type==pred_1,na.rm=T) # 0.6491228
library("class")
set.seed(2) # I am fixing the rendomstate to an arbitrary number i.e. 2
model_2 <- knn(train = train_df[,-10],test = test_df[,-10],cl = factor(train_df$type),k = 5)
pred_2 <- model_2
confusionmatrix2 <- table(test_df$type,model_2)
mean(test_df$type==model_2) # Accuracy is 0.625
# From the above two methods we can perform any
train_acc <- c();test_acc <- c()
for (i in seq(1,50,2)){
# I am fixing the rendomstate to an arbitrary number i.e. 3 dont remove it, it may lead to different answers
set.seed(2);pred_knns <- knn(train=train_df[,-10],test=train_df[,-10],cl=train_df$type,k=i)
train_acc <- c(train_acc,mean(train_df$type==pred_knns))
set.seed(2);perd_test_knn <- knn(train = train_df[,-10], test = test_df[,-10], cl = train_df$type, k=i)
test_acc <- c(test_acc,mean(perd_test_knn==test_df$type))
};test_acc
plot(seq(1,50,2),train_acc,col="red",type='b',pch=20,main="Train and Test  Accuracy",xlab = "K value",ylab = "Accuracy")
lines(seq(1,50,2),test_acc,col=ifelse(test_acc==max(test_acc),"blue","green"),type='b',pch=20,cex=ifelse(test_acc==max(test_acc),2,1))
#From this plot we can see taht train accuracy is highest for k = 1 and Test Accuracy is highest for k= 3
set.seed(2);model_final <- knn(train = train_df[,-10],test = test_df[,-10],cl = factor(train_df$type),k = 3)
pred_final <- model_final
confusionmatrixfinal <- table(test_df$type,model_final)
mean(test_df$type==model_final) # Accuracy is 0.71875
confusionmatrixfinal
Zooanimal <- read.csv("Zoo.csv")
head(Zooanimal)
table(Zooanimal$type)
table(Zooanimal$type)
Zoo <- data.frame("animal"=Zooanimal$animal.name,"type"=Zooanimal$type,normalize_dummy(Zooanimal[,-c(1,18)]))
# there are 7 types of animal in the Zoo
set.seed(101)
splits<- sample(x = 1:nrow(Zoo),size = round(nrow(Zoo)*.3),replace = F)
Zoo_train <- Zoo[-splits,]
Zoo_test <- Zoo[splits,]
barplot(Zooanimal$type)
Zooanimal$type
barplot(table(Zooanimal$type))
barplot(table(Zooanimal$type),col = 1:7)
barplot(table(Zooanimal$type),col = "forestgreen",xlab = )
barplot(table(Zooanimal$type),col = "forestgreen",xlab = "Types of animal",ylab = "Count")
str(Zooanimal)
summary(Zooanimal)
barplot(table(Zooanimal$aquatic),col = "forestgreen",xlab = "Types of animal",ylab = "Count")
barplot(table(Zooanimal$milk),col = "forestgreen",xlab = "Types of animal",ylab = "Count")
barplot(table(Zooanimal$venomous),col = "forestgreen",xlab = "Types of animal",ylab = "Count")
barplot(table(Zooanimal$fins),col = "forestgreen",xlab = "Types of animal",ylab = "Count")
barplot(Zooanimal[,-1])
plot(Zooanimal[,-1])
barplot(table(Zooanimal$hair),col = "forestgreen",xlab = "Types of animal",ylab = "Count")
barplot(table(Zooanimal$eggs),col = "forestgreen",xlab = "Types of animal",ylab = "Count")
barplot(table(Zooanimal$airborne),col = "forestgreen",xlab = "Types of animal",ylab = "Count")
barplot(table(Zooanimal$aquatic),col = "forestgreen",xlab = "Types of animal",ylab = "Count")
nrow(Zooanimal)
plot(1:101,Zooanimal$aquatic)
plot(1:101,Zooanimal$aquatic,col=Zooanimal$hair)
plot(1:101,Zooanimal$aquatic,col=Zooanimal$hair+1)
set.seed(2) # I am fixing the rendomstate to an arbitrary number i.e. 2
model_2z <- knn(train = Zoo_train[,-(1:2)],test = Zoo_test[,-(1:2)],cl = factor(Zoo_train$type),k = 3)
pred_2z <- model_2z
confusionmatrix2 <- table(Zoo_test$type,model_2z)
mean(Zoo_test$type==model_2z) # Accuracy is 0.9
tabulate(Zooanimal$aquatic,Zooanimal$hair)
table(Zooanimal$aquatic,Zooanimal$hair)
table(aquatic=Zooanimal$aquatic,Zooanimal$hair)
table(aquatic=Zooanimal$aquatic,hair = Zooanimal$hair)
plot(1:101,Zooanimal$aquatic,col=Zooanimal$hair+2) #aquatic animal with hair
which(Zooanimal$aquatic == Zooanimal$hair)
plot(1:101,Zooanimal$aquatic,col=ifelse(Zooanimal$aquatic == Zooanimal$hair,"blue","yellow")) #aquatic animal with hair
plot(1:101,Zooanimal$aquatic,col=ifelse(Zooanimal$aquatic == Zooanimal$hair,"blue","yellow"),pch = 20) #aquatic animal with hair
plot(1:101,Zooanimal$aquatic,col=ifelse(Zooanimal$aquatic == Zooanimal$hair,"blue","yellow"),pch = 20,cex=2) #aquatic animal with hair
model_2z <- knn(train = Zoo_train[,-(1:2)],test = Zoo_test[,-(1:2)],cl = factor(Zoo_train$type),k = 3)
pred_2z <- model_2z
confusionmatrix2 <- table(Zoo_test$type,model_2z)
mean(Zoo_test$type==model_2z) # Accuracy is 0.9
confusionmatrix2
mean(Zoo_test$type==model_2z) # Accuracy is 0.9
train_zacc <- c();test_zacc <- c()
for (i in seq(1,50,2)){
# I am fixing the rendomstate to an arbitrary number i.e. 3 dont remove it, it may lead to different answers
set.seed(3)
pred_knns <- knn(train=Zoo_train[,-(1:2)],test=Zoo_train[,-(1:2)],cl=Zoo_train$type,k=i)
train_zacc <- c(train_zacc,mean(Zoo_train$type==pred_knns))
set.seed(3)
perd_test_knn <- knn(train = Zoo_train[,-(1:2)], test = Zoo_test[,-(1:2)], cl = Zoo_train$type, k=i)
test_zacc <- c(test_zacc,mean(Zoo_test$type==perd_test_knn))
};test_zacc;train_zacc
plot(seq(1,50,2),train_zacc,col="red",type='b',pch=20,main="Train and Test  Accuracy",xlab = "K value",ylab = "Accuracy")
lines(seq(1,50,2),test_zacc,col=ifelse(test_zacc==max(test_zacc),"blue","green"),type='b',pch=20,cex=ifelse(test_zacc==max(test_zacc),2,1))
legend(x = "topr",legend = c("Train Accuracy","Test Accuracy"),fill = c("red","green"),bty = "n")
set.seed(3);zoomodel <- knn(train = Zoo_train[,-(1:2)],test = Zoo_test[,-(1:2)],cl = factor(Zoo_train$type),k = 1)
confusionMat_Zoo <- table(Zoo_test$type,zoomodel)
confusionMat_Zoo
confusionMat_Zoo
confusionMat_Zoo
ggplot(data = data.frame(test_zacc,train_zacc,k= seq(1,50,2)),aes(x=k))+geom_point(aes(y =test_zacc,x=k,col="Test Accuracy"))+
geom_line(aes(y =test_zacc,x=k),col="pink")+geom_point(aes(y =train_zacc,x=k,col="Train Accuracy"))+
geom_line(aes(y =train_zacc,x=k),colour="blue")+ylab(label = "")
sum(diag(confusionMat_Zoo)/sum(confusionMat_Zoo))
sum(diag(confusionMat_Zoo)/sum(confusionMat_Zoo))
confusionMat_Zoo
boxplot(glass$Type)
barplot(glass$Type)
barplot(table(glass$Type))
barplot(table(glass$Type),col='forestgreen')
barplot(table(glass$Type),col='forestgreen',xlab="type of glass",ylab='Count')
box(df_glass)
boxplot(df_glass)
df_glass <- data.frame(glass[,-10],type=factor(glass$Type))
boxplot(df_glass)
df_glass <- data.frame(normalize_dummy(glass[,-10]),type=glass$Type)
boxplot(df_glass)
sum(diag(confusionMat_Zoo)/sum(confusionMat_Zoo))
